{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import ollama\n",
    "import time\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "\"\"\"\n",
    "line_of_code=input(\"Please enter line of code that needs explanation:\")\n",
    "question += line_of_code\n",
    "print (question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2d7fae80-1b95-4ffd-9766-7b78af72cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a technical assistant who can review a line of code and explain what this line of code does, \\\n",
    "with a technical breakdown of each piece of the code, and a one line summary in plain english at the end.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def tech_copilot_gpt(question,model):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f14430c6-ee42-499d-aa83-d68e9ff96ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down the line of code:\n",
       "\n",
       "1. **print**: This is a built-in function in Python that outputs data to the console (standard output). It can take various types of arguments, including strings, numbers, and more, and will display them as text.\n",
       "\n",
       "2. **\"Hello World!\"**: This is a string literal, which is a sequence of characters enclosed in double quotes. In this case, the string contains the text `Hello World!`, which is a common phrase used as a simple test output for programming languages.\n",
       "\n",
       "3. **()**: The parentheses are used to pass arguments to the function. In this case, the string `\"Hello World!\"` is the argument being passed to the `print` function.\n",
       "\n",
       "### Technical Breakdown\n",
       "- The `print` function is called, initiating the output process.\n",
       "- The content within the parentheses (`\"Hello World!\"`) is evaluated as a string.\n",
       "- The `print` function then takes this string and sends it to the console, where it appears as visible text for the user.\n",
       "\n",
       "### Summary\n",
       "This line of code outputs the text \"Hello World!\" to the console."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tech_copilot_gpt(question,MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "def tech_copilot_llama(question,model):\n",
    "    payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\":[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            \"stream\": True \n",
    "        }\n",
    "    \n",
    "    stream = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream.iter_lines():\n",
    "        decoded_data = chunk.decode(\"utf-8\")\n",
    "        parsed_json = json.loads(decoded_data)  # Parse JSON\n",
    "        response += parsed_json[\"message\"][\"content\"] or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "977652f1-fbdf-4a0e-bc29-8d563443cb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Technical Breakdown:**\n",
       "\n",
       "This line of code is using a combination of Python features to generate an iterable sequence of authors.\n",
       "\n",
       "Here's a step-by-step explanation:\n",
       "\n",
       "1. `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression that iterates over the `books` collection and yields each author found in the \"author\" key of the dictionaries within it.\n",
       "\t* `for book in books`: Iterates over the `books` collection, which is assumed to be an iterable (e.g., list, tuple, or dictionary).\n",
       "\t* `if book.get(\"author\")`: Filters out any dictionaries that don't have an \"author\" key. The `get()` method returns `None` if the key is not found.\n",
       "2. `yield from ...`: This keyword is used to yield a sub-iterable (in this case, another generator expression) as its own value.\n",
       "\n",
       "**How it works:**\n",
       "\n",
       "The outer `yield from` statement takes the sequence of authors generated by the inner generator expression and yields each author individually.\n",
       "\n",
       "**Plain English Summary:**\n",
       "This line of code generates an iterable sequence of book authors from a collection of books."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tech_copilot_llama(question,MODEL_LLAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e008b-f85f-4f3b-b637-a2a681c2244a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
